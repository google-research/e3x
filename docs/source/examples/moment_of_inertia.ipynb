{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12252498",
   "metadata": {},
   "source": [
    "# Moment of inertia (regression of equivariant properties)\n",
    "This tutorial demonstrates how to use E3x to construct a simple model for the prediction of equivariant properties. In this toy example, we want to predict the <a href=\"https://en.wikipedia.org/wiki/Moment_of_inertia#Inertia_tensor\">moment of inertia tensor</a> of a collection of point masses. The $3\\times 3$ inertia tensor $\\mathbf{I}$ for a collection of $N$ point masses with masses $m_i$ and positions $\\vec{r}_i = [x_i\\ y_i\\ z_i]$ is given by\n",
    "\n",
    "$$\n",
    "\\mathbf{I} = \\begin{bmatrix}\n",
    "I_{xx} & I_{xy} & I_{xz} \\\\\n",
    "I_{yx} & I_{yy} & I_{yz} \\\\\n",
    "I_{zx} & I_{zy} & I_{zz} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "with the components\n",
    "\n",
    "$$\n",
    "I_{\\alpha\\beta} = \\sum_{i=1}^{N} m_i \\left(\\lVert \\vec{r}_i \\rVert^2\\delta_{\\alpha\\beta} - \\alpha_i\\beta_i \\right)\n",
    "$$\n",
    "\n",
    "where $\\alpha$ and $\\beta$ can be either $x$, $y$, or $z$, and $\\delta_{\\alpha\\beta}$ is $1$ if $\\alpha = \\beta$ and $0$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf48f1c3",
   "metadata": {},
   "source": [
    "First, all necessary packages are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff180750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import e3x\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "# Disable future warnings.\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea093b59",
   "metadata": {},
   "source": [
    "Next, we define a function that generates a dataset by randomly drawing positions and masses and calculating the corresponding moment of inertia tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d620ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moment_of_inertia_tensor(masses, positions):\n",
    "  diag = jnp.sum(positions**2, axis=-1)[..., None, None]*jnp.eye(3)\n",
    "  outer = positions[..., None, :] * positions[..., :, None]\n",
    "  return jnp.sum(masses[..., None, None] * (diag - outer), axis=-3)\n",
    "    \n",
    "def generate_datasets(key, num_train=1000, num_valid=100, num_points=10, min_mass=0.0, max_mass=1.0, stdev=1.0):\n",
    "  # Generate random keys.\n",
    "  train_position_key, train_masses_key, valid_position_key, valid_masses_key = jax.random.split(key, num=4)\n",
    "\n",
    "  # Draw random point masses with random positions.\n",
    "  train_positions = stdev * jax.random.normal(train_position_key,  shape=(num_train, num_points, 3))\n",
    "  train_masses = jax.random.uniform(train_masses_key, shape=(num_train, num_points), minval=min_mass, maxval=max_mass)\n",
    "  valid_positions = stdev * jax.random.normal(valid_position_key,  shape=(num_valid, num_points, 3))\n",
    "  valid_masses = jax.random.uniform(valid_masses_key, shape=(num_valid, num_points), minval=min_mass, maxval=max_mass)\n",
    "  \n",
    "  # Calculate moment of inertia tensors.    \n",
    "  train_inertia_tensor = calculate_moment_of_inertia_tensor(train_masses, train_positions)\n",
    "  valid_inertia_tensor = calculate_moment_of_inertia_tensor(valid_masses, valid_positions)\n",
    "  \n",
    "  # Return final train and validation datasets.\n",
    "  train_data = dict(positions=train_positions, masses=train_masses, inertia_tensor=train_inertia_tensor)\n",
    "  valid_data = dict(positions=valid_positions, masses=valid_masses, inertia_tensor=valid_inertia_tensor)\n",
    "  return train_data, valid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60e6658",
   "metadata": {},
   "source": [
    "We now define an equivariant model to solve this regression task using the modules in E3x. The architecture takes as input masses and positions and outputs a $3\\times3$ matrix, or rather, a second order tensor. It comprises the following steps:\n",
    "\n",
    "1. Initialize features by concatenating masses and positions and reshaping to match the feature shape conventions used in E3x.\n",
    "\n",
    "2. Apply the following transformations: First we project the mass-position features to a `features`-dimensional feature space using a `Dense` layer. Next, a `TensorDense` layer is applied to allow coupling between the irreps $\\mathbb{0}$ (scalars) and $\\mathbb{1}$ (vectors). A second `TensorDense` layer is applied, because in general, to predict an arbitrary second order tensor, we need (even) irreps $\\mathbb{0}$, $\\mathbb{1}$, and $\\mathbb{2}$ (since $\\mathbb{1} \\otimes \\mathbb{1} = \\mathbb{0}\\oplus\\mathbb{1}\\oplus\\mathbb{2}$). Thus, the features are \"elevated\" from maximum degree $1$ (scalars and vectors) to `max_degree=2`. Further, since we only want to predict a single second order tensor, the layer also maps from the `features`-dimensional feature space to single output irreps $\\mathbb{0}$, $\\mathbb{1}$, and $\\mathbb{2}$ (`features=1`). Note: Since the moment of inertia tensor is symmetric, it really only consists of irreps $\\mathbb{0}$ and $\\mathbb{2}$. We could thus zero out the irrep of degree $1$ to only predict symmetric tensors. However, let's pretend that we do not know this, the model should learn to predict (almost) symmetric tensors anyway.\n",
    "\n",
    "3. Sum over contributions from individual points.\n",
    "\n",
    "4. Build the $3\\times3$ tensor from the irreps by applying the Clebsch-Gordan rule backwards: $\\mathbb{0}\\oplus\\mathbb{1}\\oplus\\mathbb{2} = \\mathbb{1} \\otimes \\mathbb{1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24120e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "  features = 8\n",
    "  max_degree = 1\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, masses, positions):  # Shapes (..., N) and (..., N, 3).\n",
    "    # 1. Initialize features.\n",
    "    x = jnp.concatenate((masses[..., None], positions), axis=-1) # Shape (..., N, 4).\n",
    "    x = x[..., None, :, None]  # Shape (..., N, 1, 4, 1).\n",
    "    \n",
    "    # 2. Apply transformations.\n",
    "    x = e3x.nn.Dense(features=self.features)(x)  # Shape (..., N, 1, 4, features).\n",
    "    x = e3x.nn.TensorDense(max_degree=self.max_degree)(x)  # Shape (..., N, 2, (max_degree+1)**2, features).\n",
    "    x = e3x.nn.TensorDense(  # Shape (..., N, 2, 9, 1).\n",
    "        features=1,\n",
    "        max_degree=2, \n",
    "    )(x)\n",
    "    # Try it: Zero-out irrep of degree 1 to only produce symmetric output tensors.\n",
    "    # x = x.at[..., :, 1:4, :].set(0)\n",
    "    \n",
    "    # 3. Collect even irreps from feature channel 0 and sum over contributions from individual points.\n",
    "    x = jnp.sum(x[..., 0, :, 0], axis=-2)  # Shape (..., (max_degree+1)**2).\n",
    "    \n",
    "    # 4. Convert output irreps to 3x3 matrix and return.\n",
    "    cg = e3x.so3.clebsch_gordan(max_degree1=1, max_degree2=1, max_degree3=2)  # Shape (4, 4, 9).\n",
    "    y = jnp.einsum('...l,nml->...nm', x, cg[1:, 1:, :])  # Shape (..., 3, 3).\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de343b4e",
   "metadata": {},
   "source": [
    "Next, we define our loss function. As is common for regression tasks, we choose the $L_2$ (squared error) loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffdb6656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(prediction, target):\n",
    "  return jnp.mean(optax.l2_loss(prediction, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e01fe49",
   "metadata": {},
   "source": [
    "Now that we have all ingredients, let's write some boilerplate for training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a941e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'optimizer_update'))\n",
    "def train_step(model_apply, optimizer_update, batch, opt_state, params):\n",
    "  def loss_fn(params):\n",
    "    inertia_tensor = model_apply(params, batch['masses'], batch['positions'])\n",
    "    loss = mean_squared_loss(inertia_tensor, batch['inertia_tensor'])\n",
    "    return loss\n",
    "  loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "  updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return params, opt_state, loss\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('model_apply',))\n",
    "def eval_step(model_apply, batch, params):\n",
    "  inertia_tensor = model_apply(params, batch['masses'], batch['positions'])\n",
    "  loss = mean_squared_loss(inertia_tensor, batch['inertia_tensor'])\n",
    "  return loss\n",
    "\n",
    "def train_model(key, model, train_data, valid_data, num_epochs, learning_rate, batch_size):\n",
    "  # Initialize model parameters and optimizer state.\n",
    "  key, init_key = jax.random.split(key)\n",
    "  optimizer = optax.adam(learning_rate)\n",
    "  params = model.init(init_key, train_data['masses'][0:1], train_data['positions'][0:1])\n",
    "  opt_state = optimizer.init(params)\n",
    "    \n",
    "  # Determine the number of training steps per epoch.\n",
    "  train_size = len(train_data['masses'])\n",
    "  steps_per_epoch = train_size//batch_size\n",
    "\n",
    "  # Train for 'num_epochs' epochs.\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    perms = jax.random.permutation(shuffle_key, train_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "    \n",
    "    # Loop over all batches.\n",
    "    train_loss = 0.0  # For keeping a running average of the loss.\n",
    "    for i, perm in enumerate(perms):\n",
    "      batch = {k: v[perm, ...] for k, v in train_data.items()}\n",
    "      params, opt_state, loss = train_step(\n",
    "          model_apply=model.apply, \n",
    "          optimizer_update=optimizer.update, \n",
    "          batch=batch, \n",
    "          opt_state=opt_state, \n",
    "          params=params\n",
    "      )\n",
    "      train_loss += (loss - train_loss)/(i+1)\n",
    "        \n",
    "    # Evaluate on the test set after each training epoch.\n",
    "    valid_loss = eval_step(\n",
    "        model_apply=model.apply, \n",
    "        batch=valid_data,\n",
    "        params=params\n",
    "    )\n",
    "    \n",
    "    # Print progress.\n",
    "    print(f\"epoch {epoch : 4d} train loss {train_loss : 8.6f} valid loss {valid_loss : 8.6f}\")\n",
    "\n",
    "  # Return final model parameters.\n",
    "  return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d4ce78",
   "metadata": {},
   "source": [
    "Finally, let's create our toy dataset and choose the training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f730b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRNGKey for random number generation.\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Generate train and test datasets. \n",
    "key, data_key = jax.random.split(key)\n",
    "train_data, valid_data = generate_datasets(data_key)\n",
    "\n",
    "# Define training hyperparameters.\n",
    "learning_rate = 0.002\n",
    "num_epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead1eb4a",
   "metadata": {},
   "source": [
    "Now, we train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af6f3f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    1 train loss  1.359933 valid loss  0.650806\n",
      "epoch    2 train loss  0.471154 valid loss  0.361696\n",
      "epoch    3 train loss  0.355795 valid loss  0.330646\n",
      "epoch    4 train loss  0.335975 valid loss  0.313806\n",
      "epoch    5 train loss  0.313707 valid loss  0.307905\n",
      "epoch    6 train loss  0.295819 valid loss  0.261203\n",
      "epoch    7 train loss  0.269274 valid loss  0.236152\n",
      "epoch    8 train loss  0.247977 valid loss  0.230414\n",
      "epoch    9 train loss  0.231734 valid loss  0.205375\n",
      "epoch   10 train loss  0.225083 valid loss  0.209193\n",
      "epoch   11 train loss  0.207602 valid loss  0.188981\n",
      "epoch   12 train loss  0.200761 valid loss  0.185399\n",
      "epoch   13 train loss  0.190793 valid loss  0.175384\n",
      "epoch   14 train loss  0.178643 valid loss  0.169232\n",
      "epoch   15 train loss  0.161587 valid loss  0.144264\n",
      "epoch   16 train loss  0.147181 valid loss  0.133549\n",
      "epoch   17 train loss  0.129426 valid loss  0.109603\n",
      "epoch   18 train loss  0.105608 valid loss  0.088042\n",
      "epoch   19 train loss  0.089911 valid loss  0.065109\n",
      "epoch   20 train loss  0.063822 valid loss  0.046581\n",
      "epoch   21 train loss  0.042836 valid loss  0.039833\n",
      "epoch   22 train loss  0.045359 valid loss  0.037879\n",
      "epoch   23 train loss  0.040164 valid loss  0.048566\n",
      "epoch   24 train loss  0.041613 valid loss  0.038852\n",
      "epoch   25 train loss  0.037659 valid loss  0.036376\n",
      "epoch   26 train loss  0.034417 valid loss  0.038344\n",
      "epoch   27 train loss  0.035188 valid loss  0.030785\n",
      "epoch   28 train loss  0.033791 valid loss  0.031197\n",
      "epoch   29 train loss  0.033894 valid loss  0.027737\n",
      "epoch   30 train loss  0.033661 valid loss  0.030736\n",
      "epoch   31 train loss  0.031063 valid loss  0.025989\n",
      "epoch   32 train loss  0.029492 valid loss  0.026486\n",
      "epoch   33 train loss  0.029381 valid loss  0.024327\n",
      "epoch   34 train loss  0.028494 valid loss  0.023874\n",
      "epoch   35 train loss  0.029745 valid loss  0.028929\n",
      "epoch   36 train loss  0.028127 valid loss  0.023014\n",
      "epoch   37 train loss  0.026591 valid loss  0.024094\n",
      "epoch   38 train loss  0.030150 valid loss  0.028875\n",
      "epoch   39 train loss  0.029794 valid loss  0.023051\n",
      "epoch   40 train loss  0.031116 valid loss  0.028424\n",
      "epoch   41 train loss  0.028212 valid loss  0.021998\n",
      "epoch   42 train loss  0.025479 valid loss  0.021279\n",
      "epoch   43 train loss  0.025208 valid loss  0.024125\n",
      "epoch   44 train loss  0.026061 valid loss  0.021150\n",
      "epoch   45 train loss  0.033841 valid loss  0.057384\n",
      "epoch   46 train loss  0.027158 valid loss  0.020290\n",
      "epoch   47 train loss  0.023987 valid loss  0.019953\n",
      "epoch   48 train loss  0.024759 valid loss  0.024593\n",
      "epoch   49 train loss  0.025928 valid loss  0.024374\n",
      "epoch   50 train loss  0.023460 valid loss  0.018815\n",
      "epoch   51 train loss  0.024572 valid loss  0.019993\n",
      "epoch   52 train loss  0.022887 valid loss  0.018372\n",
      "epoch   53 train loss  0.026181 valid loss  0.025382\n",
      "epoch   54 train loss  0.025671 valid loss  0.021562\n",
      "epoch   55 train loss  0.023084 valid loss  0.017371\n",
      "epoch   56 train loss  0.024710 valid loss  0.019425\n",
      "epoch   57 train loss  0.029084 valid loss  0.029276\n",
      "epoch   58 train loss  0.022432 valid loss  0.015814\n",
      "epoch   59 train loss  0.020230 valid loss  0.016195\n",
      "epoch   60 train loss  0.018062 valid loss  0.014702\n",
      "epoch   61 train loss  0.016403 valid loss  0.013027\n",
      "epoch   62 train loss  0.016155 valid loss  0.012115\n",
      "epoch   63 train loss  0.013847 valid loss  0.010955\n",
      "epoch   64 train loss  0.014790 valid loss  0.011325\n",
      "epoch   65 train loss  0.019166 valid loss  0.010374\n",
      "epoch   66 train loss  0.012386 valid loss  0.007711\n",
      "epoch   67 train loss  0.009181 valid loss  0.006591\n",
      "epoch   68 train loss  0.006980 valid loss  0.005574\n",
      "epoch   69 train loss  0.005331 valid loss  0.004197\n",
      "epoch   70 train loss  0.006010 valid loss  0.013546\n",
      "epoch   71 train loss  0.004674 valid loss  0.004104\n",
      "epoch   72 train loss  0.002438 valid loss  0.001107\n",
      "epoch   73 train loss  0.001398 valid loss  0.000977\n",
      "epoch   74 train loss  0.001085 valid loss  0.000696\n",
      "epoch   75 train loss  0.000974 valid loss  0.000871\n",
      "epoch   76 train loss  0.000811 valid loss  0.000642\n",
      "epoch   77 train loss  0.000723 valid loss  0.000854\n",
      "epoch   78 train loss  0.000840 valid loss  0.001953\n",
      "epoch   79 train loss  0.000751 valid loss  0.000519\n",
      "epoch   80 train loss  0.000636 valid loss  0.000593\n",
      "epoch   81 train loss  0.000609 valid loss  0.000431\n",
      "epoch   82 train loss  0.000456 valid loss  0.000393\n",
      "epoch   83 train loss  0.000414 valid loss  0.000432\n",
      "epoch   84 train loss  0.000414 valid loss  0.000325\n",
      "epoch   85 train loss  0.000374 valid loss  0.000378\n",
      "epoch   86 train loss  0.000338 valid loss  0.000319\n",
      "epoch   87 train loss  0.000391 valid loss  0.000368\n",
      "epoch   88 train loss  0.000297 valid loss  0.000303\n",
      "epoch   89 train loss  0.000289 valid loss  0.000255\n",
      "epoch   90 train loss  0.000279 valid loss  0.000331\n",
      "epoch   91 train loss  0.000330 valid loss  0.000231\n",
      "epoch   92 train loss  0.000239 valid loss  0.000275\n",
      "epoch   93 train loss  0.000244 valid loss  0.000312\n",
      "epoch   94 train loss  0.000205 valid loss  0.000246\n",
      "epoch   95 train loss  0.000211 valid loss  0.000307\n",
      "epoch   96 train loss  0.000207 valid loss  0.000279\n",
      "epoch   97 train loss  0.000195 valid loss  0.000168\n",
      "epoch   98 train loss  0.000177 valid loss  0.000205\n",
      "epoch   99 train loss  0.000346 valid loss  0.000148\n",
      "epoch  100 train loss  0.000156 valid loss  0.000148\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "key, train_key = jax.random.split(key)\n",
    "model = Model()\n",
    "params = train_model(\n",
    "  key=train_key,\n",
    "  model=model,\n",
    "  train_data=train_data,\n",
    "  valid_data=valid_data,\n",
    "  num_epochs=num_epochs,\n",
    "  learning_rate=learning_rate, \n",
    "  batch_size=batch_size, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad6b6a8",
   "metadata": {},
   "source": [
    "The loss goes down very quickly. With longer training, it would be possible to eventually reach a loss of virtually zero. However, the current value seems low enough. Let's verify that our model really predicts the correct moment of inertia tensor by evaluating it on an entry from the validation set and comparing with the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74f38b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "[[ 6.013584    1.6290329  -0.17871115]\n",
      " [ 1.6290329   4.8540945   0.73430276]\n",
      " [-0.17871115  0.73430276  6.1854286 ]]\n",
      "prediction\n",
      "[[ 6.0166373   1.6341103  -0.17790869]\n",
      " [ 1.634112    4.849129    0.73786813]\n",
      " [-0.1779084   0.73786616  6.182283  ]]\n",
      "mean squared error 1.3572028e-05\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "masses, positions, target = valid_data['masses'][i], valid_data['positions'][i], valid_data['inertia_tensor'][i]\n",
    "prediction = model.apply(params, masses, positions)\n",
    "\n",
    "print('target')\n",
    "print(target)\n",
    "print('prediction')\n",
    "print(prediction)\n",
    "print('mean squared error', jnp.mean((prediction-target)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957beecd",
   "metadata": {},
   "source": [
    "That looks pretty good! But is our model really equivariant? Let's try to randomly rotate the input positions. The output of our model should rotate accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14dc20ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rotated target\n",
      "[[ 6.4829473  1.0275574  1.0426031]\n",
      " [ 1.0275574  4.921893  -0.9980936]\n",
      " [ 1.0426031 -0.9980936  5.6482677]]\n",
      "rotated prediction\n",
      "[[ 6.487672   1.0301298  1.0456917]\n",
      " [ 1.0301319  4.914525  -1.0000154]\n",
      " [ 1.0456927 -1.0000156  5.645855 ]]\n",
      "mean squared error 1.3572109e-05\n"
     ]
    }
   ],
   "source": [
    "key, rotation_key = jax.random.split(key)\n",
    "rotation = e3x.so3.random_rotation(rotation_key)\n",
    "rotated_positions = positions@rotation\n",
    "rotated_target = calculate_moment_of_inertia_tensor(masses, rotated_positions)\n",
    "rotated_prediction = model.apply(params, masses, rotated_positions)\n",
    "\n",
    "print('rotated target')\n",
    "print(rotated_target)\n",
    "print('rotated prediction')\n",
    "print(rotated_prediction)\n",
    "print('mean squared error', jnp.mean((rotated_prediction-rotated_target)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48adb156",
   "metadata": {},
   "source": [
    "Notice that the individual entries of the moment of inertia tensor have changed quite a bit, but the prediction is still just as good (up to very small differences due to imprecisions of floating point arithmetic). This is the power of equivariant models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
